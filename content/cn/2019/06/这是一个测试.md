---
title: "这是一个测试"
date: 2019-06-21T20:45:31+08:00
draft: false
markup: mmark
---
今天再次看到贝叶斯反馈公式，又想到了程序语言中类的继承、多态与封装有了很多思考。

 

**1.获得想法的场景**

普通的线性拟合，目标是最小化目标函数，目标函数为：

最小二乘：

>   f(x)=∑(y-yhat)\^2

最大似然：

-   概率密度函数：

$$
f\left( e_{i};\theta \right) = \frac{1}{\sigma\sqrt{2\pi}}e^{{- \frac{1}{2}\left( \frac{e_{i}}{\sigma} \right)}^{2}} = \frac{1}{\sigma\sqrt{2\pi}}e^{- \frac{1}{2}\left( \frac{y_{i} - {yh\text{at}}_{i}}{\sigma} \right)^{2}}
$$

-   似然函数

$$
\text{likeli}h\text{ood} = \prod_{i = 1}^{\text{Nobs}}{\frac{1}{\sigma\sqrt{2\pi}}e^{- \frac{1}{2}\left( \frac{y_{i} - {yh\text{at}}_{i}}{\sigma} \right)^{2}}}
$$

-   负的对数转换后的似然函数：

$$
- \operatorname{LL=-ln}\left( \text{likeli}h\text{ood} \right) = - \sum_{i = 1}^{\text{Nobs}}{\ln\left( \frac{1}{\sigma\sqrt{2\pi}} \right) -}\frac{1}{2}\left( \frac{y_{i} - {yh\text{at}}_{i}}{\sigma} \right)^{2} = \sum_{i = 1}^{\text{Nobs}}{\ln\left( \sigma\sqrt{2\pi} \right) +}\frac{1}{2}\left( \frac{y_{i} - {yh\text{at}}_{i}}{\sigma} \right)^{2}
$$

-   去掉常数项并乘2：

$$
\operatorname{-2LL=}{\sum_{i = 1}^{\text{Nobs}}\left( \frac{y_{i} - {yh\text{at}}_{i}}{\sigma} \right)^{2}}
$$

 

而贝叶斯反馈的公式为：

$$
f\left( x \right) = \sum_{j = 1}^{n}\left( \frac{p_{i} - {ph\text{at}}_{i}}{\omega} \right)^{2} + \sum_{i = 1}^{m}\left( \frac{c_{i} - {ch\text{at}}_{i}}{\sigma} \right)^{2}
$$

贝叶斯反馈公式额外考虑了普通的回归方程中的参数的分布！

 

**2.猜想，**

那是否普通的线性拟合其实也考虑了参数的先验分布呢？

 

**3.计算猜想**

普通线性回归时，因为对参数一无所知，可以假设参数时均匀分布的，我们随机抽取到一组参数的概率是完全相等的。

即认为普通的线性拟合的参数分布是均匀分布，由此按照贝叶斯反馈公式的思路推导普通的线性拟合的方程。

先验信息：

-   均匀分布的概率密度函数为：

>   f(x)=1/(a-b)

-   似然函数为：

$$
L(\theta|x) = \prod_{}^{}\frac{1}{a - b} = \left( a - b \right)^{- n}
$$

-   \-2LL为：

$$
- 2\text{LL} = n \times \ln\left( a - b \right)
$$

-   其中a,b为自变量的定义域，是常数，所以-2LL是一个常数项

 

 

则普通线性回归的贝叶斯反馈公式为：

$$
f\left( x \right) = n \times \ln(a - b) + \sum_{i = 1}^{m}\left( \frac{c_{i} - {ch\text{at}}_{i}}{\sigma} \right)^{2}
$$

方程的含义：

普通线性回归时，首先抽取按照均匀分布抽取获得一组参数值，抽取获得这组参数值得概率为一个常数，然后有了参数值后，使用参数值可求得因变量预测值，带入方程，可以求得目标函数，之后最小化该目标函数即可从均匀分布的参数中寻找到最符合观测点的一组参数。

由此可以看到，其实普通线性回归时也考虑的参数的先验分布，并假设未来参数的先验分布为均匀分布。

 

**4.继续猜想**

这样子看，其实可以把贝叶斯反馈看做是普通线性回归这个类的一个子类，并且贝叶斯反馈对参数先验分布进行了重构，改为了对数正态分布或正态分布，体现了类的多态。

$$
f\left( x \right) = \sum_{j = 1}^{n}\left( \frac{p_{i} - {ph\text{at}}_{i}}{\omega} \right)^{2} + \sum_{i = 1}^{m}\left( \frac{c_{i} - {ch\text{at}}_{i}}{\sigma} \right)^{2}
$$

这个贝叶斯反馈公式可以看做是使用了“回归参数服从正太分布"这个假设，然后进行的最小化。

（这里都应用和包含了一个额外的假设，线性回归的参数与观测值的残差相互独立）

 

**5.接着应用**

既然

>   普通的线性回归可以看做是：

>   1.参数有先验的分布，

>   2.从先验分布中抽取值带入公式求解目标函数

>   贝叶斯反馈公式是普通的线性回归的一个子类，并且发生了重构、多态。

那么，可否进一步的在抽象一层：

-   贝叶斯反馈公式中参数的分布参数也是符合一种分布的，在不知道的情况下直接继承父类的均匀分布，

-   由这种先验的分布中随机抽取得到一组分布的参数，

-   然后在从分布中抽取得到一组参数，

-   使用参数求取方程后半部分的值，进而获得整个目标函数。

为了清晰，这里需要引入一些符号区分不同的参数。

>   令θ表示线性回归的所有参数（比如θ可能表示：θ1=int,θ1=k），

>   令φ表示θ的分布参数（比如，这里假设φ表示正态分布，φ可能表示为φ1=mean,φ2=ω，而θ\~N(mean,ω)，即θ的分布使用φ表示），

>   令ψ表示φ的分布参数（比如，这里假设ψ表示均匀分布，ψ可能表示为ψ1=a,ψ2=b，而φ\~(a,b)，即θ的分布使用φ表示）

则之前的语句可以重写为：

-   贝叶斯反馈公式中参数的分布参数也是符合一种分布的，在不知道的情况下直接继承父类的均匀分布，该分部可用ψ表示，

-   由这种先验的分布ψ中，随机抽取得到一组分布的参数φ，

-   然后在从φ分布中抽取得到一组参数θ，

-   使用参数θ求取因变量预测值，获得方程后半部分的值，进而获得整个目标函数。

 

**6.可继续如此无限的抽象**

可继续如此无限的抽象，比如在把ψ重构为某种分布，然后在从某种分布中抽取ψ，如此可以无限的向上不断的重构，直到再往前就全部是均匀分布未知（因为均匀分布得到的是常数项，在最小化时可以被忽略）

 

**7.意义**

在第5部分的抽象时，可以看到已经得到了我常见非线性混合效应模型（线性不知道）（起码我这样认为）。

 
